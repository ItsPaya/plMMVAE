{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class LitVAE(pl.LightningModule):\n",
    "    def __init__(self, alpha = 1):\n",
    "        #Autoencoder only requires 1 dimensional argument since input and output-size is the same\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(784,196),nn.ReLU(),nn.BatchNorm1d(196,momentum = 0.7),\n",
    "                                     nn.Linear(196,49),nn.ReLU(),nn.BatchNorm1d(49,momentum = 0.7),\n",
    "                                     nn.Linear(49,28),nn.LeakyReLU())\n",
    "        self.hidden2mu = nn.Linear(28,28)\n",
    "        self.hidden2log_var = nn.Linear(28,28)\n",
    "        self.alpha = alpha\n",
    "        self.decoder = nn.Sequential(nn.Linear(28,49),nn.ReLU(),\n",
    "                                     nn.Linear(49,196),nn.ReLU(),\n",
    "                                     nn.Linear(196,784),nn.Tanh())\n",
    "        self.data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "\n",
    "    def encode(self,x):\n",
    "       hidden = self.encoder(x)\n",
    "       mu = self.hidden2mu(hidden)\n",
    "       log_var = self.hidden2log_var(hidden)\n",
    "       return mu,log_var\n",
    "\n",
    "    def decode(self,x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def reparametrize(self,mu,log_var):\n",
    "        #Reparametrization Trick to allow gradients to backpropagate from the\n",
    "        #stochastic part of the model\n",
    "        sigma = torch.exp(0.5*log_var)\n",
    "        z = torch.randn(size = (mu.size(0),mu.size(1)))\n",
    "        z= z.type_as(mu) # Setting z to be .cuda when using GPU training\n",
    "        return mu + sigma*z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        mu, log_var = self.encode(x)\n",
    "\n",
    "        kl_loss =  (-0.5*(1+log_var - mu**2- torch.exp(log_var)).sum(dim = 1)).mean(dim =0)\n",
    "        hidden = self.reparametrize(mu, log_var)\n",
    "        x_out = self.decode(hidden)\n",
    "\n",
    "        recon_loss_criterion = nn.MSELoss() #Reconstruction Loss\n",
    "        recon_loss = recon_loss_criterion(x,x_out)\n",
    "        loss = recon_loss*self.alpha + kl_loss\n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        mu, log_var = self.encode(x)\n",
    "\n",
    "        kl_loss =  (-0.5*(1+log_var - mu**2- torch.exp(log_var)).sum(dim = 1)).mean(dim =0)\n",
    "        hidden = self.reparametrize(mu, log_var)\n",
    "        x_out = self.decode(hidden)\n",
    "\n",
    "        recon_loss_criterion = nn.MSELoss() #Reconstruction Loss\n",
    "        recon_loss = recon_loss_criterion(x,x_out)\n",
    "        loss = recon_loss*self.alpha + kl_loss\n",
    "\n",
    "        self.log('val_kl_loss', kl_loss, on_step=True, on_epoch=True)\n",
    "        self.log('val_recon_loss', recon_loss, on_step=True, on_epoch=True)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "        return x_out, loss\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        if not os.path.exists('vae_images'):\n",
    "            os.makedirs('vae_images')\n",
    "        choice = random.choice(outputs)\n",
    "        output_sample = choice[0]\n",
    "        output_sample = output_sample.reshape(-1,1,28,28)\n",
    "        output_sample = self.scale_image(output_sample)\n",
    "        utils.save_image(output_sample, f\"vae_images/epoch_{self.current_epoch+1}.png\")\n",
    "\n",
    "    def forward(self,x):\n",
    "       batch_size = x.size(0)\n",
    "       x = x.view(batch_size,-1)\n",
    "       mu,log_var = self.encode(x)\n",
    "       hidden = self.reparametrize(mu,log_var)\n",
    "       return self.decoder(hidden)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9912422 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1140ae8e73d54630808f4fa2f06192e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/28881 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be4c7279774b42bf9ed0b74412606115"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1648877 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27686ef7a96d43f9a0cb89f698fc6980"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f177ffae8e684932967a0cd737e7ec41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unwrapping the module did not yield a `LightningModule`, got <class 'type'> instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    649\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 650\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m \u001B[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:733\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    731\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[1;32m    732\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_ckpt_path(\n\u001B[0;32m--> 733\u001B[0m     ckpt_path, model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    734\u001B[0m )\n\u001B[1;32m    735\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mckpt_path)\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:2124\u001B[0m, in \u001B[0;36mTrainer.lightning_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2121\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   2122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlightning_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2123\u001B[0m     \u001B[38;5;66;03m# TODO: this is actually an optional return\u001B[39;00m\n\u001B[0;32m-> 2124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:335\u001B[0m, in \u001B[0;36mStrategy.lightning_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the pure LightningModule without potential wrappers.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 335\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43munwrap_lightning_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py:110\u001B[0m, in \u001B[0;36munwrap_lightning_module\u001B[0;34m(wrapped_model)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[0;32m--> 110\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnwrapping the module did not yield a `LightningModule`, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[0;31mTypeError\u001B[0m: Unwrapping the module did not yield a `LightningModule`, got <class 'type'> instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [5], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m val_loader \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(mnist_val, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)\n\u001B[1;32m      9\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(auto_lr_find\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m25\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mLitVAE\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:696\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    677\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;124;03mRuns the full optimization routine.\u001B[39;00m\n\u001B[1;32m    679\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001B[39;00m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 696\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:663\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    660\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m distributed_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworld_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    661\u001B[0m     \u001B[38;5;66;03m# try syncing remaining processes, kill otherwise\u001B[39;00m\n\u001B[1;32m    662\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mreconciliate_processes(traceback\u001B[38;5;241m.\u001B[39mformat_exc())\n\u001B[0;32m--> 663\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_callback_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mon_exception\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexception\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    664\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n\u001B[1;32m    665\u001B[0m \u001B[38;5;66;03m# teardown might access the stage so we reset it after\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1588\u001B[0m, in \u001B[0;36mTrainer._call_callback_hooks\u001B[0;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1585\u001B[0m             fn(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1586\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m-> 1588\u001B[0m pl_module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n\u001B[1;32m   1589\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pl_module:\n\u001B[1;32m   1590\u001B[0m     prev_fx_name \u001B[38;5;241m=\u001B[39m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:2124\u001B[0m, in \u001B[0;36mTrainer.lightning_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2121\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   2122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlightning_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2123\u001B[0m     \u001B[38;5;66;03m# TODO: this is actually an optional return\u001B[39;00m\n\u001B[0;32m-> 2124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:335\u001B[0m, in \u001B[0;36mStrategy.lightning_module\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlightning_module\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the pure LightningModule without potential wrappers.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 335\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43munwrap_lightning_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProject/LightTest/venv/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py:110\u001B[0m, in \u001B[0;36munwrap_lightning_module\u001B[0;34m(wrapped_model)\u001B[0m\n\u001B[1;32m    108\u001B[0m     model \u001B[38;5;241m=\u001B[39m unwrap_lightning_module(model\u001B[38;5;241m.\u001B[39mmodule)\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule):\n\u001B[0;32m--> 110\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnwrapping the module did not yield a `LightningModule`, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[0;31mTypeError\u001B[0m: Unwrapping the module did not yield a `LightningModule`, got <class 'type'> instead."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "mnist_train = MNIST('data/', download=True, train=True, transform=transform)\n",
    "train_dataloader = utils.data.DataLoader(mnist_train, batch_size=64)\n",
    "\n",
    "mnist_val = MNIST('data/', download=True, train=False, transform=transform)\n",
    "val_loader = utils.data.DataLoader(mnist_val, batch_size=64)\n",
    "\n",
    "trainer = pl.Trainer(auto_lr_find=True, max_epochs=25)\n",
    "trainer.fit(LitVAE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}